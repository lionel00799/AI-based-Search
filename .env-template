# 1 - Search
# Options: searxng, tavily, serper, bing
SEARCH_PROVIDER=searxng
SEARXNG_BASE_URL=http://searxng:8080

# tavily, serper, bing (Optional)
TAVILY_API_KEY=
SERPER_API_KEY=
BING_API_KEY=

# 2 - LLMs
OLLAMA_API_BASE=http://host.docker.internal:11434

# Cloud Models (Optional)
OPENAI_API_KEY=
GROQ_API_KEY=

AZURE_DEPLOYMENT_NAME=
AZURE_API_KEY=
AZURE_API_BASE=
AZURE_API_VERSION=

# azure, openai
OPENAI_MODE=openai

# Any `provider/model` from https://litellm.vercel.app/docs/providers
CUSTOM_MODEL=

# 3 - Frontend
NEXT_PUBLIC_API_URL=http://localhost:8000

# DB
DATABASE_URL=postgresql+psycopg2://postgres:password@db:5432/postgres
DB_ENABLED=True


# 4 - Caching + Rate Limiting (Optional)
RATE_LIMIT_ENABLED=False
REDIS_URL=


# 5 - Local Models
ENABLE_LOCAL_MODELS=True
NEXT_PUBLIC_LOCAL_MODE_ENABLED=True
NEXT_PUBLIC_PRO_MODE_ENABLED=True
